<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models">
  <meta name="keywords" content="PHYBench, Large Language Models, Physical Perception, Physical Reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- MathJax -->
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start navbar-centered">
        <a title="home" ="navbar-item" href="https://stephenqsstarthomas.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item">
              Coming Soon
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://stephenqsstarthomas.github.io">Shi Qiu</a><sup>1</sup>,</span>
                <span class="author-block">Shaoyang Guo<sup>1</sup>,</span>
                <span class="author-block"><a href="https://github.com/SonnyNondegeneracy">Zhuo-Yang Song</a> <sup>1</sup>,</span>
                <span class="author-block"><a href="https://renko6626.github.io">Yunbo Sun</a><sup>1</sup>,</span>
                <span class="author-block"><a href="https://github.com/parkcai">Zeyu Cai</a><sup>1</sup>,</span>
                <span class="author-block"><a href="https://github.com/wjsoj">Jiashen Wei</a><sup>1</sup>,</span>
                <span class="author-block"><a href="https://github.com/Dennisbroo">Tianyu Luo</a><sup>1</sup>,</span>
                <span class="author-block">Yixuan Yin<sup>1</sup>,</span>
                <span class="author-block">Haoxu Zhang<sup>1</sup>,</span>
                <span class="author-block"><a href="https://aheadofpotato.github.io">Yi Hu</a><sup>2</sup>,</span>
                <span class="author-block">Chenyang Wang<sup>1</sup>,</span>
                <span class="author-block">Chencheng Tang<sup>1</sup>,</span>
                <span class="author-block">Haoling Chang<sup>1</sup>,</span>
                <span class="author-block">Qi Liu<sup>1</sup>,</span>
                <span class="author-block">Ziheng Zhou<sup>1</sup>,</span>
                <span class="author-block">Tianyu Zhang<sup>1</sup>,</span>
                <span class="author-block">Jingtian Zhang<sup>1</sup>,</span>
                <span class="author-block">Zhangyi Liu<sup>1</sup>,</span>
                <span class="author-block">Minghao Li<sup>1</sup>,</span>
                <span class="author-block">Yuku Zhang<sup>1</sup>,</span>
                <span class="author-block">Boxuan Jing<sup>1</sup>,</span>
                <span class="author-block">Xianqi Yin<sup>1</sup>,</span>
                <span class="author-block">Yutong Ren<sup>1</sup>,</span>
                <span class="author-block">Zizhuo Fu<sup>2</sup>,</span>
                <span class="author-block">Weike Wang<sup>1</sup>,</span>
                <span class="author-block">Xudong Tian<sup>1</sup>,</span>
                <span class="author-block">Anqi Lv<sup>1</sup>,</span>
                <span class="author-block">Laifu Man<sup>1</sup>,</span>
                <span class="author-block">Jianxiang Li<sup>1</sup>,</span>
                <span class="author-block">Feiyu Tao<sup>1</sup>,</span>
                <span class="author-block">Qihua Sun<sup>1</sup>,</span>
                <span class="author-block">Zhou Liang<sup>1</sup>,</span>
                <span class="author-block">Yushu Mu<sup>1</sup>,</span>
                <span class="author-block">Zhongxuan Li<sup>1</sup>,</span>
                <span class="author-block">Jing-Jun Zhang<sup>1</sup>,</span>
                <span class="author-block">Shutao Zhang<sup>1</sup>,</span>
                <span class="author-block">Xiaotian Li<sup>1</sup>,</span>
                <span class="author-block">Xingqi Xia<sup>1</sup>,</span>
                <span class="author-block">Jiawei Lin<sup>1</sup>,</span>
                <span class="author-block">Zheyu Shen<sup>1</sup>,</span>
                <span class="author-block">Jiahang Chen<sup>1</sup>,</span>
                <span class="author-block">Qiuhao Xiong<sup>1</sup>,</span>
                <span class="author-block">Binran Wang<sup>1</sup>,</span>
                <span class="author-block">Fengyuan Wang<sup>1</sup>,</span>
                <span class="author-block">Ziyang Ni<sup>1</sup>,</span>
                <span class="author-block">Bohan Zhang<sup>5</sup>,</span>
                <span class="author-block">Fan Cui<sup>4</sup>,</span>
                <span class="author-block">Changkun Shao<sup>1</sup>,</span>
                <span class="author-block"><a href="https://faculty.pku.edu.cn/caoqinghong/zh_CN/index.htm">Qing-Hong Cao</a><sup>1</sup>,</span>
                <span class="author-block"><a href="https://www.csrc.ac.cn/en/people/faculty/184.html">Ming-Xing Luo</a> <sup>3</sup>,</span>
                <span class="author-block"><a href="https://muhanzhang.github.io">Muhan Zhang</a><sup>2</sup>,</span>
                <span class="author-block"><a href="https://konformal.github.io">Hua Xing Zhu</a><sup>1</sup>,</span>
              
            </div>
            

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>School of Physics, Peking University</span><br>
              <span class="author-block"><sup>2</sup>Institute for Artificial Intelligence, Peking University</span><br>
              <span class="author-block"><sup>3</sup>Beijing Computational Science Research Center</span><br>
              <span class="author-block"><sup>4</sup>School of Integrated Circuits, Peking University</span><br>
              <span class="author-block"><sup>5</sup>Yuanpei College, Peking University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2504.16074" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2504.16074" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/phybench-official" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://xxxxxxxxx" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="hero-body-image">
          <img src="./static/images/test_example.jpg" class="teaser-image" alt="Teaser image">
        </div>
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">PHYBench</span> is a benchmark for evaluating the physical reasoning capabilities of large
          language models.
        </h2>
      </div>
    </div>
  </section>

  
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">🌟 Overview</h2>
          <div class="content has-text-justified">
            <p><strong>PHYBench</strong> is the first large-scale benchmark specifically designed to evaluate <strong>physical perception</strong> and <strong>robust reasoning</strong> capabilities in Large Language Models (LLMs).</p>
            <p>With <strong>500 meticulously curated physics problems</strong> spanning mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, it challenges models to demonstrate:</p>
            <ul>
              <li><strong>Real-world grounding</strong>: Problems based on tangible physical scenarios (e.g., ball inside a bowl, pendulum dynamics)</li>
              <li><strong>Multi-step reasoning</strong>: Average solution length of 3,000 characters requiring 10+ intermediate steps</li>
              <li><strong>Symbolic precision</strong>: Strict evaluation of LaTeX-formatted expressions through novel <strong>Expression Edit Distance (EED) Score</strong></li>
            </ul>
            <h3 class="title is-4">Key innovations:</h3>
            <ul>
              <li>🎯 <strong>EED Metric</strong>: Continuous scoring (0-100) measuring expression tree similarity, capturing partial correctness</li>
              <li>🏋️ <strong>Difficulty Spectrum</strong>: High school, undergraduate, Physics Olympiad-level problems</li>
              <li>🔍 <strong>Error Taxonomy</strong>: Explicit evaluation of Physical Perception (PP) vs Robust Reasoning (RR) failures</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>
  



  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">📚 Example Problems</h2>
          <div id="problem-carousel" class="carousel carousel-animated carousel-animate-slide" data-autoplay="false">
            <div class="carousel-item is-active">
              <img src="./static/images/problem1.jpg" alt="Problem 1">
            </div>
            <div class="carousel-item">
              <img src="./static/images/problem2.jpg" alt="Problem 2">
            </div>
            <div class="carousel-item">
              <img src="./static/images/problem3.jpg" alt="Problem 3">
            </div>
            <div class="carousel-item">
              <img src="./static/images/problem4.jpg" alt="Problem 4">
            </div>
            <div class="carousel-item">
              <img src="./static/images/problem5.jpg" alt="Problem 5">
            </div>
          </div>
          
          <!-- TODO: clicking "prev" button twice on prob 2 will cause a bug where the carousel will stick at prob 1 -->
          <script>
            document.addEventListener('DOMContentLoaded', () => {
              const carousel = bulmaCarousel.attach('#problem-carousel', {
                slidesToScroll: 1,
                slidesToShow: 1,
                navigation: true,
                pagination: false,
                loop: false,
                autoplay: false,
                duration: 500,
                timing: 'cubic-bezier(0.22, 0.61, 0.36, 1)',
              })[0];
            
              const navLeft  = carousel.element.querySelector('.carousel-nav-left');
              const navRight = carousel.element.querySelector('.carousel-nav-right');
            
              const lock   = () => { navLeft.style.pointerEvents = navRight.style.pointerEvents = 'none'; };
              const unlock = () => { navLeft.style.pointerEvents = navRight.style.pointerEvents = 'auto'; };
            
              unlock();                            // 初始可点击
              carousel.on('before:show', lock);    // 动画开始时锁定
              carousel.on('after:show', unlock);   // 动画结束时解锁
            });
            </script>
            
            
          <style>
            #problem-carousel{
              position:relative;
              overflow:hidden;
            }
            #problem-carousel::before,
            #problem-carousel::after{
              content:"";
              position:absolute;
              top:0;
              width:30px;
              height:100%;
              pointer-events:none;
              z-index:2;
            }
            #problem-carousel::before{
              left: 0px;
              background:linear-gradient(to right,#fff 0%,rgba(255,255,255,0) 100%);
            }
            #problem-carousel::after{
              right: 0px;
              background:linear-gradient(to left,#fff 0%,rgba(255,255,255,0) 100%);
            }
            #problem-carousel .carousel-item{
              padding:0 1.5rem;
            }
          </style>

          <h3 class="title is-4 has-text-left">Answer Types</h3>
          <div class="content has-text-left">
            <ul>
              <li> Strict symbolic expressions (e.g., <span>$\sqrt{\frac{2g}{3R}}$</span>)</li>
              <li> Multiple equivalent forms accepted</li>
              <li> No numerical approximations or equation chains</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="hero-body-image">
          <img src="./static/images/framework.jpg" class="teaser-image" alt="Teaser image">
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">🛠️ Data Curation</h2>
          <div class="content has-text-justified">
            <h3 class="title is-4">3-Stage Rigorous Validation Pipeline</h3>

            <h4>1. Expert Creation & Strict Screening</h4>
            <ul>
              <li><strong>178 PKU physics students</strong> contributed problems that are:
                <ul>
                  <li>Almost entirely original/custom-created</li>
                  <li>None easily found through direct internet searches or standard reference materials</li>
                </ul>
              </li>
              <li>Strict requirements:
                <ul>
                  <li>✅ Single unambiguous symbolic answer (e.g., \( T=2mg+4mv_0^2/l \))</li>
                  <li>✉️ Text-only solvability (no diagrams/multimodal inputs)</li>
                  <li>Rigorously precise statements to avoid ambiguity</li>
                  <li>Solvable using only basic physics principles (no complex specialized knowledge required)</li>
                </ul>
              </li>
              <li>No requirements on AI test to avoid filtering for AI weaknesses</li>
            </ul>

            <h4>2. Multi-Round Academic Review</h4>
            <p>Dedicated internal platform for peer review:</p>
            <img src="https://example.com/review-platform.png" alt="Review Interface">
            <p><strong>3-tier verification process:</strong></p>
            <ul>
              <li>Initial filtering: Reviewers assessed format validity and appropriateness (not filtering for AI weaknesses)</li>
              <li>Ambiguity detection and revision: Reviewers analyzed LLM-generated solutions to identify potential ambiguities in problem statements</li>
              <li>Iterative improvement cycle: Questions refined repeatedly until all LLMs can understand the question and follow the instructions to produce the expressions it believes to be right.</li>
            </ul>

            <h4>3. Human Expert Finalization</h4>
            <ul>
              <li><strong>81 PKU students participated:</strong></li>
              <li>Each student independently solved 8 problems from the dataset</li>
              <li>Evaluate question clarity, statement rigor, and answer correctness</li>
              <li>Establish of human baseline performance meanwhile</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">📊 Evaluation Protocol</h2>
          <div class="content has-text-justified">
            <h3 class="title is-4">Machine Evaluation</h3>
            <p><strong>Dual Metrics</strong>:</p>
            <ol>
              <li><strong>Accuracy</strong>: Binary correctness (expression equivalence via SymPy simplification)</li>
              <li><strong>EED Score</strong>: Continuous assessment of expression tree similarity</li>
            </ol>
            <p>The EED Score evaluates the similarity between the model-generated answer and the ground truth by leveraging the concept of expression tree edit distance. The process involves the following steps:</p>
            <ol>
              <li><strong>Simplification of Expressions</strong>: Both the ground truth (`gt`) and the model-generated answer (`gen`) are first converted into simplified symbolic expressions using the `sympy.simplify()` function. This step ensures that equivalent forms of the same expression are recognized as identical.</li>
              <li><strong>Equivalence Check</strong>: If the simplified expressions of `gt` and `gen` are identical, the EED Score is assigned a perfect score of 100, indicating complete correctness.</li>
              <li><strong>Tree Conversion and Edit Distance Calculation</strong>: If the expressions are not identical, they are converted into tree structures. The edit distance between these trees is then calculated using an extended version of the Zhang-Shasha algorithm. This distance represents the minimum number of node-level operations (insertions, deletions, and updates) required to transform one tree into the other.</li>
              <li><strong>Relative Edit Distance and Scoring</strong>: The relative edit distance \( r \) is computed as the ratio of the edit distance to the size of the ground truth tree. The EED Score is then determined based on this relative distance:
                <ul>
                  <li>If \( r = 0 \) (i.e., the expressions are identical), the score is 100.</li>
                  <li>If \( 0 < r < 0.6 \), the score is calculated as \( 60 - 100r \).</li>
                  <li>If \( r \geq 0.6 \), the score is 0, indicating a significant discrepancy between the model-generated answer and the ground truth.</li>
                </ul>
              </li>
            </ol>
            <p><strong>Key Advantages</strong>:</p>
            <ul>
              <li>204% higher sample efficiency vs binary metrics</li>
              <li>Distinguishes coefficient errors (30&lt;EED score&lt;60) vs structural errors (EED score&lt;30)</li>
            </ul>

            <h3 class="title is-4">Human Baseline</h3>
            <ul>
              <li><strong>Participants</strong>: 81 PKU physics students</li>
              <li><strong>Protocol</strong>:
                <ul>
                  <li>8 problems per student: Each student solved a set of 8 problems from PHYBench dataset</li>
                  <li>Time-constrained solving: 3 hours</li>
                </ul>
              </li>
              <li><strong>Performance metrics</strong>:
                <ul>
                  <li>61.9±2.1% average accuracy</li>
                  <li>70.4±1.8 average EED Score</li>
                  <li>Top quartile reached 71.4% accuracy and 80.4 EED Score</li>
                  <li>Significant outperformance vs LLMs: Human experts outperformed all evaluated LLMs at 99% confidence level</li>
                </ul>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">📝 Main Results</h2>
          <div class="content has-text-justified">
            <p>The results of the evaluation are shown in the following figure:</p>
            <img src="./static/docs/figures/fig3.png" alt="Evaluation Results" style="max-width:100%;margin-bottom:1.5rem;">
            <ol>
              <li><strong>Significant Performance Gap</strong>: Even state-of-the-art LLMs significantly lag behind human experts in physical reasoning. The highest-performing model, Gemini 2.5 Pro, achieved only a 36.9% accuracy, compared to the human baseline of 61.9%.</li>
              <li><strong>EED Score Advantages</strong>: The EED Score provides a more nuanced evaluation of model performance compared to traditional binary scoring methods.</li>
              <li><strong>Domain-Specific Strengths</strong>: Different models exhibit varying strengths in different domains of physics:</li>
            </ol>
            <img src="./static/docs/figures/fig4-a.png" alt="Domain Performance" style="max-width:100%;margin-bottom:1.5rem;">
            <ul>
              <li>Gemini 2.5 Pro shows strong performance across most domains</li>
              <li>DeepSeek-R1 and o3-mini (high) show comparable performance in mechanics and electricity</li>
              <li>Most models struggle with advanced physics and modern physics</li>
            </ul>
            <p><strong>Difficulty Handling</strong>: Comparing the advantage across problem difficulties, Gemini 2.5 Pro gains a pronounced edge on harder problems, followed by o3 (high).</p>
            <img src="./static/docs/figures/fig4-b.png" alt="Difficulty Performance" style="max-width:100%;margin-bottom:1.5rem;">
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">😵‍💫 Error Analysis</h2>
          <div class="content has-text-justified">
            <img src="./static/docs/figures/fig5.png" alt="Error Analysis" style="max-width:100%;margin-bottom:1.5rem;">
            <p>We categorize the capabilities assessed by the PHYBench benchmark into two key dimensions: <strong>Physical Perception (PP)</strong> and <strong>Robust Reasoning (RR)</strong>:</p>
            <ol>
              <li><strong>Physical Perception (PP) Errors</strong>: During this phase, models engage in intensive semantic reasoning, expending significant cognitive effort to identify relevant physical objects, variables, and dynamics. Models make qualitative judgments about which physical effects are significant and which can be safely ignored. PP manifests as critical decision nodes in the reasoning chain. An example of a PP error is shown in Example Problem 1.</li>
              <li><strong>Robust Reasoning (RR) Errors</strong>: In this phase, models produce numerous lines of equations and perform symbolic reasoning. This process forms the connecting chains between perception nodes. RR involves consistent mathematical derivation, equation solving, and proper application of established conditions. An example of a RR error is shown in Example Problem 2.</li>
            </ol>
            <img src="./static/docs/figures/box1-example_reasoning_process.png" alt="Error Example" style="max-width:100%;margin-bottom:1.5rem;">
          </div>
        </div>
      </div>
    </div>
  </section>

<!-- Citation section (Bulma) -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">🚩 Citation</h2>

        <!-- ① 自带背景与内边距；② 横向滚动避免溢出 -->
        <pre class="has-background-light p-4" style="overflow-x:auto;">
<code class="language-bibtex">@misc{qiu2025phybenchholisticevaluationphysical,
  title        = {PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models},
  author       = {Shi Qiu and Shaoyang Guo and Zhuo-Yang Song and Yunbo Sun and Zeyu Cai and Jiashen Wei and Tianyu Luo and Yixuan Yin and Haoxu Zhang and Yi Hu and Chenyang Wang and Chencheng Tang and Haoling Chang and Qi Liu and Ziheng Zhou and Tianyu Zhang and Jingtian Zhang and Zhangyi Liu and Minghao Li and Yuku Zhang and Boxuan Jing and Xianqi Yin and Yutong Ren and Zizhuo Fu and Weike Wang and Xudong Tian and Anqi Lv and Laifu Man and Jianxiang Li and Feiyu Tao and Qihua Sun and Zhou Liang and Yushu Mu and Zhongxuan Li and Jing-Jun Zhang and Shutao Zhang and Xiaotian Li and Xingqi Xia and Jiawei Lin and Zheyu Shen and Jiahang Chen and Qiuhao Xiong and Binran Wang and Fengyuan Wang and Ziyang Ni and Bohan Zhang and Fan Cui and Changkun Shao and Qing-Hong Cao and Ming-xing Luo and Muhan Zhang and Hua Xing Zhu},
  year         = {2025},
  eprint       = {2504.16074},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2504.16074}
}</code></pre>
      </div>
    </div>
  </div>
</section>

<!-- 额外的小样式，可放到全局 CSS -->
<style>
  /* 等宽字体 + 更紧凑字号，易读且不撑版 */
  pre code.language-bibtex {
    font-family: "Fira Code", monospace;
    font-size: 0.85rem;
    line-height: 1.35;
  }
</style>

  
  

</body>

</html>
